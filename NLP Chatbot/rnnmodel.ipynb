{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies for RNN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, metrics\n",
    "from utils import separate_dataset, build_dataset, str_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\Anuj\\Documents\\GitHub\\Natural Language Processing\\NLP Chatbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = datasets.load_files(container_path = './sentiment data', encoding = 'utf-8')\n",
    "traindata.data , traindata.target = separate_dataset(traindata, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative', 'Positive']\n"
     ]
    }
   ],
   "source": [
    "print(traindata.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the dataset into ONE HOT Encoding\n",
    "ONEHOT = np.zeros((len(traindata.data), len(traindata.target_names)))\n",
    "ONEHOT[np.arange(len(traindata.data)), traindata.target] = 1.0\n",
    "train_X, test_X, train_Y, test_Y, train_onehot, test_onehot = train_test_split(\n",
    "    traindata.data,\n",
    "    traindata.target,\n",
    "    ONEHOT, test_size=0.2\n",
    ")\n",
    "concat = ' '.join(traindata.data).split()\n",
    "#print(concat)\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 13997\n",
      "Most common words [('the', 10132), ('a', 6936), ('of', 5502), ('and', 5294), ('to', 4528), ('is', 3334)]\n",
      "Sample data [683, 197, 7, 370, 12, 36, 6243, 7, 2067, 51] ['simplistic', 'silly', 'and', 'tedious', 'its', 'so', 'laddish', 'and', 'juvenile', 'only'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('vocab from size:', vocabulary_size)\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag to mark the beginning of the sentence\n",
    "GO = dictionary['GO']       # 0th position\n",
    "# Tag to add extra padding in the sentence\n",
    "PAD = dictionary['PAD']     # 1st position\n",
    "# Tag to mark the end of the sentence\n",
    "EOS = dictionary['EOS']     # 2nd position\n",
    "# Tag to mark the unknown word\n",
    "UNK = dictionary['UNK']     # 3rd position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 128 #The number of units in RNN cell\n",
    "num_layers = 2 #The number of hidden layers\n",
    "embedded_size = 128 #The size of embeddding\n",
    "dimension_output = len(traindata.target_names) # Number of classes  \n",
    "learning_rate = 1e-3 # The lr of the optimization algorithm\n",
    "maxlen = 50 \n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size,\n",
    "                 dict_size, dimension_output, learning_rate):\n",
    "\n",
    "        def cells(reuse=False):\n",
    "            return tf.nn.rnn_cell.BasicRNNCell(size_layer, reuse=reuse)\n",
    "        '''\n",
    "        2 place holder: 1. to feed the sequence data to the model\n",
    "                        2. for the output\n",
    "        '''\n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
    "\n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n",
    "        '''\n",
    "        Variable to store the embedded lookup for the dictionary\n",
    "        '''\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "        '''\n",
    "        Add the RNN Layer\n",
    "        '''\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n",
    "        outputs, _ = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded, dtype=tf.float32)\n",
    "        '''\n",
    "        Create weights and Bias\n",
    "        '''\n",
    "        W = tf.get_variable('w',\n",
    "                            shape=(size_layer, dimension_output),\n",
    "                            initializer=tf.orthogonal_initializer())\n",
    "        b = tf.get_variable('b',\n",
    "                            shape=(dimension_output),\n",
    "                            initializer=tf.zeros_initializer())\n",
    "        '''\n",
    "        logits are computed by performing a matrix multiplication of the weight, \n",
    "        the output from the RNN layer, and addition of bias\n",
    "        '''\n",
    "        self.logits = tf.matmul(outputs[:, -1], W) + b\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y)\n",
    "        )\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anuj\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.510742\n",
      "time taken: 10.172515392303467\n",
      "epoch: 0, training loss: 0.732301, training acc: 0.448982, valid loss: 0.702369, valid acc: 0.510742\n",
      "\n",
      "time taken: 6.98599910736084\n",
      "epoch: 1, training loss: 0.707971, training acc: 0.489347, valid loss: 0.706188, valid acc: 0.506836\n",
      "\n",
      "epoch: 2, pass acc: 0.510742, current acc: 0.516602\n",
      "time taken: 10.062481880187988\n",
      "epoch: 2, training loss: 0.700831, training acc: 0.525450, valid loss: 0.722394, valid acc: 0.516602\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anuj\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "time taken: 10.400992393493652\n",
      "epoch: 3, training loss: 0.688384, training acc: 0.559422, valid loss: 0.728733, valid acc: 0.501465\n",
      "\n",
      "time taken: 7.053036689758301\n",
      "epoch: 4, training loss: 0.658756, training acc: 0.612926, valid loss: 0.759279, valid acc: 0.502930\n",
      "\n",
      "time taken: 9.946547746658325\n",
      "epoch: 5, training loss: 0.632482, training acc: 0.646307, valid loss: 0.791856, valid acc: 0.516113\n",
      "\n",
      "time taken: 7.644988298416138\n",
      "epoch: 6, training loss: 0.609265, training acc: 0.664536, valid loss: 0.802313, valid acc: 0.516602\n",
      "\n",
      "epoch: 7, pass acc: 0.516602, current acc: 0.526855\n",
      "time taken: 7.283017158508301\n",
      "epoch: 7, training loss: 0.617016, training acc: 0.663589, valid loss: 0.789467, valid acc: 0.526855\n",
      "\n",
      "time taken: 7.0469958782196045\n",
      "epoch: 8, training loss: 0.644724, training acc: 0.621567, valid loss: 0.719766, valid acc: 0.493652\n",
      "\n",
      "time taken: 9.900521516799927\n",
      "epoch: 9, training loss: 0.642395, training acc: 0.640743, valid loss: 0.725119, valid acc: 0.497559\n",
      "\n",
      "time taken: 7.282994985580444\n",
      "epoch: 10, training loss: 0.663883, training acc: 0.611742, valid loss: 0.764915, valid acc: 0.509766\n",
      "\n",
      "time taken: 10.056517362594604\n",
      "epoch: 11, training loss: 0.696184, training acc: 0.555516, valid loss: 0.716567, valid acc: 0.494141\n",
      "\n",
      "time taken: 9.73947811126709\n",
      "epoch: 12, training loss: 0.700072, training acc: 0.500355, valid loss: 0.716188, valid acc: 0.494141\n",
      "\n",
      "break epoch:13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(size_layer, num_layers, embedded_size,\n",
    "              vocabulary_size + 4, dimension_output,\n",
    "              learning_rate)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=2)\n",
    "checkpoint_dir = os.path.abspath(os.path.join('./', \"checkpoints_basic_rnn\"))\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 5, 0, 0, 0\n",
    "\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n' % (EPOCH))\n",
    "        break\n",
    "\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    for i in range(0, (len(train_X) // batch_size) * batch_size, batch_size):\n",
    "        batch_x = str_idx(train_X[i: i + batch_size], dictionary, maxlen)\n",
    "        acc, loss, _ = sess.run(\n",
    "            [model.accuracy, model.cost, model.optimizer],\n",
    "            feed_dict={\n",
    "                model.X: batch_x,\n",
    "                model.Y: train_onehot[i: i + batch_size]\n",
    "            }\n",
    "        )\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "\n",
    "    for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):\n",
    "        batch_x = str_idx(test_X[i: i + batch_size], dictionary, maxlen)\n",
    "        acc, loss = sess.run(\n",
    "            [model.accuracy, model.cost],\n",
    "            feed_dict={\n",
    "                model.X: batch_x,\n",
    "                model.Y: train_onehot[i: i + batch_size]\n",
    "            }\n",
    "        )\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "\n",
    "    train_loss /= (len(train_X) // batch_size)\n",
    "    train_acc /= (len(train_X) // batch_size)\n",
    "    test_loss /= (len(test_X) // batch_size)\n",
    "    test_acc /= (len(test_X) // batch_size)\n",
    "\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print('epoch: %d, pass acc: %f, current acc: %f' % (EPOCH, CURRENT_ACC, test_acc))\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "\n",
    "    print('time taken:', time.time()-lasttime)\n",
    "    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n' %\n",
    "          (EPOCH, train_loss, train_acc, test_loss, test_acc))\n",
    "    path = saver.save(sess, checkpoint_prefix, global_step=EPOCH)\n",
    "    EPOCH += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.49      1.00      0.66      1053\n",
      "    Positive       0.00      0.00      0.00      1080\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      2133\n",
      "   macro avg       0.25      0.50      0.33      2133\n",
      "weighted avg       0.24      0.49      0.33      2133\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anuj\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "logits = sess.run(model.logits,\n",
    "                  feed_dict={model.X: str_idx(test_X, dictionary, maxlen)})\n",
    "print(metrics.classification_report(\n",
    "    test_Y,\n",
    "    np.argmax(logits, 1),\n",
    "    target_names=traindata.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
