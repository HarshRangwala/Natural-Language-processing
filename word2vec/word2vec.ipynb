{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK tokenizer models (only the first time)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\", \" \", raw)\n",
    "    words = clean.split()\n",
    "    return list(map(lambda x: x.lower(), words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'there', 'this', 'is', 'first', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_to_wordlist('Hello there, this is first sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 425633 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Download text from Gutenberg website\n",
    "filepath = 'http://www.gutenberg.org/files/33224/33224-0.txt'\n",
    "corpus_raw = requests.get(filepath).text\n",
    "\n",
    "# Clean text\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)\n",
    "\n",
    "# Sentence where each word is tokenized\n",
    "sentences = (sentence_to_wordlist(raw) for raw in raw_sentences if raw)\n",
    "sentences = list(sentences)\n",
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(f'The book corpus contains {token_count} tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More dimensions, more computationally expensive to train\n",
    "# but also more accurate -- more dimensions = more generalized\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel\n",
    "# The more workers, the faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words. 0 to 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "seed = 1\n",
    "\n",
    "model2vec = w2v.Word2Vec(\n",
    "    sg=1, seed=seed, workers=num_workers, size=num_features,\n",
    "    min_count=min_word_count, window=context_size, sample=downsampling\n",
    ")\n",
    "\n",
    "model2vec.build_vocab(list(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Anuj\\\\Documents\\\\GitHub\\\\Natural Language Processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training, this might take a minute or two...\n",
    "model2vec.train(\n",
    "    sentences, total_examples=model2vec.corpus_count, epochs=10\n",
    ")\n",
    "\n",
    "# Save to file, can be useful later\n",
    "if not os.path.exists(os.path.join('trained', 'sample')):\n",
    "    os.makedirs(os.path.join('trained', 'sample'))\n",
    "\n",
    "model2vec.save(os.path.join('trained', 'sample', 'sample.w2v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the model\n",
    "Now that we have trained our word2vec model, let's explore what our model was able to learn.\n",
    "We will use most_similar() to explore the relations between various words. \n",
    "In the following example, you see that the model was able to learn that\n",
    "the word earth is related to crust, globe, and other words.\n",
    "It is interesting to see that we only provided the raw data and the model \n",
    "was able to learn all of these relations and concepts automatically! The following is the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to \"earth\":\n",
      "('crust', 0.7226534485816956)\n",
      "('globe', 0.6591298580169678)\n",
      "('inequalities', 0.6219401955604553)\n",
      "('planet', 0.6030153036117554)\n",
      "('orbit', 0.5927286148071289)\n",
      "('laboring', 0.567184329032898)\n",
      "('moon', 0.5649024248123169)\n",
      "('unevenness', 0.5619377493858337)\n",
      "('remodelled', 0.5601164102554321)\n",
      "('reduce', 0.5525733828544617)\n",
      "\n",
      "Most similar to \"human\":\n",
      "('man', 0.6671838760375977)\n",
      "('art', 0.6557581424713135)\n",
      "('race', 0.6433838605880737)\n",
      "('industry', 0.6433637142181396)\n",
      "('rude', 0.637246310710907)\n",
      "('gods', 0.6188982725143433)\n",
      "('affairs', 0.6096276044845581)\n",
      "('beings', 0.6006961464881897)\n",
      "('comparative', 0.5970616340637207)\n",
      "('population', 0.590903103351593)\n",
      "\n",
      "Positive words contribute positively towards similarity, negative words negatively:\n",
      "('sound', 0.815029501914978)\n",
      "('remodelled', 0.800189197063446)\n",
      "('crust', 0.7973071932792664)\n",
      "('employed', 0.795359194278717)\n",
      "('planet', 0.7855542898178101)\n",
      "('globe', 0.7845449447631836)\n",
      "('laboring', 0.7828304767608643)\n",
      "('sun', 0.7820301055908203)\n",
      "('autobiography', 0.7802436351776123)\n",
      "('attendant', 0.779236376285553)\n"
     ]
    }
   ],
   "source": [
    "# Analyzing the model\n",
    "print('Most similar to \"earth\":')\n",
    "for s in model2vec.wv.most_similar(\"earth\"):\n",
    "    print(s)\n",
    "    \n",
    "print('\\nMost similar to \"human\":')\n",
    "for s in model2vec.wv.most_similar(\"human\"):\n",
    "    print(s)\n",
    "    \n",
    "print('\\nPositive words contribute positively towards similarity, negative words negatively:')\n",
    "for s in model2vec.wv.most_similar_cosmul(\n",
    "        positive=['earth', 'moon'],\n",
    "        negative=['orbit']):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality using t-SNE\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0) #2-D space output\n",
    "all_word_vectors_matrix = model2vec.wv.vectors\n",
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to store words and coordinates\n",
    "points = pd.DataFrame(\n",
    "    [(word, coords[0], coords[1]) for word, coords in\n",
    "        [(word, all_word_vectors_matrix_2d[model2vec.wv.vocab[word].index])\n",
    "            for word in model2vec.wv.vocab]\n",
    "     ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
